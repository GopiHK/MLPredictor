{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This demonstrates the following:\n",
    "How to load imdb data\n",
    "How load converts words into integer indexes\n",
    "How words are stored in highest occuring freq to lowest occurring\n",
    "How to take top 5000 or 10000 vocbualry size\n",
    "idiosyncracy of imdb.get_word_index() \n",
    "Re-convert the index to take into account padding, start, out-of-vocabulary\n",
    "What is an embedding layer\n",
    "Create a ConvNet model\n",
    "Create a RNN model\n",
    "Create a LSTM model\n",
    "DO the sentiment analyzer\n",
    "Predict sample sentences\n",
    "Then this takes into advanced featues of Keras such as callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import Dense,  Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB dataset  25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('IMDB dataset  {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Features: review in number sequence After padding---\n",
      "[1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 2, 5940, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 9363, 1117, 1831, 7485, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 8564, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 7175, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 5390, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "Sentiment 1 = positve, 0 = negative\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('---Features: review in number sequence After padding---')\n",
    "print(X_train[6])\n",
    "print('Sentiment 1 = positve, 0 = negative')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584 the 1\n"
     ]
    }
   ],
   "source": [
    "w2id = imdb.get_word_index() \n",
    "id2word = {i: word  for word, i in w2id.items()}\n",
    "print (len(w2id), id2word[1], w2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " type(w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88582"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=[word.lower()  for word, i in w2id.items()]\n",
    "len(set(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boiled full involving to impressive boring this as murdering naschy br villain and suggestion need has of costumes b message to may of props this and concentrates concept issue skeptical to god's he is and unfolds movie women like isn't surely i'm and to toward in here's for from did having because very quality it is and starship really book is both too worked carl of and br of reviewer closer figure really there will originals things is far this make mistakes and was couldn't of few br of you to don't female than place she to was between that nothing dose movies get are and br yes female just its because many br of overly to descent people time very bland \n",
      "And sentiment is  1\n"
     ]
    }
   ],
   "source": [
    "#Jibberish data?\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 88587\n",
      "__START__ __UNK__ the and a voorhees'\n",
      "__START__ lavish production values and solid performances in this straightforward adaption of jane __UNK__ satirical classic about the marriage game within and between the classes in __UNK__ 18th century england northam and paltrow are a __UNK__ mixture as friends who must pass through __UNK__ and lies to discover that they love each other good humor is a __UNK__ virtue which goes a long way towards explaining the __UNK__ of the aged source material which has been toned down a bit in its harsh __UNK__ i liked the look of the film and how shots were set up and i thought it didn't rely too much on __UNK__ of head shots like most other films of the 80s and 90s do very good results \n",
      "And sentiment is  1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "word2id ={w: i+3 for w, i in w2id.items()}\n",
    "word2id[\"__PADDING__\"] = 0\n",
    "word2id[\"__START__\"] = 1\n",
    "word2id[\"__UNK__\"] = 2\n",
    "\n",
    "#This returns the index of the words from 1 to n with 1 being the most frequently occuring word, \n",
    "\n",
    "# and n the least frequently occuring word\n",
    "\n",
    "print (type(word2id), len (word2id))\n",
    "\n",
    "id2word = {i: word  for word, i in word2id.items()}\n",
    "print(id2word[1], id2word[2], #id2word[3],#\n",
    "      id2word[4], id2word[5], id2word[6], id2word[88586])\n",
    "actual_data = []\n",
    "for i in range (len(X_train[6])):\n",
    "    ind = X_train[6][i]\n",
    "    if(ind != 0):\n",
    "        actual_data.append(id2word[ind])\n",
    "actual_data = \" \".join (actual_data )\n",
    "print (actual_data, \"\\nAnd sentiment is \", y_train[6])\n",
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 2697\n"
     ]
    }
   ],
   "source": [
    "print('Maximum review length: {}'.format(\n",
    "len((max((X_train + X_test), key=len)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "modelCNN = None\n",
    "modelRNN=None\n",
    "modelLSTM=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 496, 128)          32128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 95, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 19, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 745,633\n",
      "Trainable params: 745,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "#embedding_size=32\n",
    "embedding_size=50\n",
    "modelCNN=Sequential()\n",
    "modelCNN.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelCNN.add(Conv1D(128, 5, activation= \"relu\"))\n",
    "modelCNN.add(MaxPooling1D(5))\n",
    "modelCNN.add(Conv1D(128, 5, activation= \"relu\"))\n",
    "modelCNN.add(MaxPooling1D(5))\n",
    "modelCNN.add(Conv1D(128, 5, activation= \"relu\"))\n",
    "modelCNN.add(MaxPooling1D(5))\n",
    "modelCNN.add(Flatten())\n",
    "modelCNN.add(Dense(128, activation='relu'))\n",
    "modelCNN.add(Dense(1, activation='sigmoid'))\n",
    "print(modelCNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/3\n",
      "24744/24744 [==============================] - 318s 13ms/step - loss: 0.5796 - acc: 0.6472 - val_loss: 0.2601 - val_acc: 0.8984\n",
      "Epoch 2/3\n",
      "24744/24744 [==============================] - 318s 13ms/step - loss: 0.2536 - acc: 0.8984 - val_loss: 0.2238 - val_acc: 0.9102\n",
      "Epoch 3/3\n",
      "24744/24744 [==============================] - 319s 13ms/step - loss: 0.1534 - acc: 0.9454 - val_loss: 0.2222 - val_acc: 0.9102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09b7c1dcc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.86652\n"
     ]
    }
   ],
   "source": [
    "scores = modelCNN.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now create a simple RNN model and lets see the accuracy\n",
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 530,301\n",
      "Trainable params: 530,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size=50\n",
    "modelRNN=Sequential()\n",
    "modelRNN.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelRNN.add(SimpleRNN(150,activation = \"relu\")) \n",
    "modelRNN.add(Dense(1, activation='sigmoid'))\n",
    "print(modelRNN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRNN.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/3\n",
      "24744/24744 [==============================] - 271s 11ms/step - loss: 0.7531 - acc: 0.6009 - val_loss: 0.6621 - val_acc: 0.5469\n",
      "Epoch 2/3\n",
      "24744/24744 [==============================] - 271s 11ms/step - loss: 0.6009 - acc: 0.6546 - val_loss: 0.5890 - val_acc: 0.6328\n",
      "Epoch 3/3\n",
      "24744/24744 [==============================] - 271s 11ms/step - loss: 0.5348 - acc: 0.7241 - val_loss: 0.5485 - val_acc: 0.6992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09aefaf240>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelRNN.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.67704\n"
     ]
    }
   ],
   "source": [
    "scoresRNN = modelRNN.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scoresRNN[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 50)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 150)               120600    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 620,751\n",
      "Trainable params: 620,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#LEts now create an LSTM model\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "embedding_size=50\n",
    "modelLSTM=Sequential()\n",
    "modelLSTM.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "modelLSTM.add(LSTM(150, dropout=0.2)) \n",
    "modelLSTM.add(Dense(1, activation='sigmoid'))\n",
    "print(modelLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24744 samples, validate on 256 samples\n",
      "Epoch 1/3\n",
      "24744/24744 [==============================] - 1121s 45ms/step - loss: 0.6163 - acc: 0.6591 - val_loss: 0.3228 - val_acc: 0.8828\n",
      "Epoch 2/3\n",
      "24744/24744 [==============================] - 1124s 45ms/step - loss: 0.3063 - acc: 0.8718 - val_loss: 0.2283 - val_acc: 0.9102\n",
      "Epoch 3/3\n",
      "24744/24744 [==============================] - 1123s 45ms/step - loss: 0.2105 - acc: 0.9189 - val_loss: 0.2069 - val_acc: 0.9141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09b1908d30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelLSTM.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.87928\n"
     ]
    }
   ],
   "source": [
    "scores = modelLSTM.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now going to predict these reviews\n"
     ]
    }
   ],
   "source": [
    "reviewList = [\"the movie was not so bad\", \n",
    "              \"the movie was a total waste of my time\",\n",
    "              \"the food was so deliciously delicious that i felt sinfully wicked\"                 \n",
    "             ]\n",
    "print (\"Now going to predict these reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print (word2id[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review= the movie was not so bad\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= not id= 24\n",
      "word= so id= 38\n",
      "word= bad id= 78\n",
      "Prediction Probability for  RNN  =  0.12519452 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  CNN  =  0.62546253 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  LSTM  =  0.38893726 Sentiment= Negative \n",
      "\n",
      "review= the movie was a total waste of my time\n",
      "word= the id= 4\n",
      "word= movie id= 20\n",
      "word= was id= 16\n",
      "word= a id= 6\n",
      "word= total id= 964\n",
      "word= waste id= 437\n",
      "word= of id= 7\n",
      "word= my id= 61\n",
      "word= time id= 58\n",
      "Prediction Probability for  RNN  =  0.08136006 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  CNN  =  0.16423544 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  LSTM  =  0.20100501 Sentiment= Negative \n",
      "\n",
      "review= the food was so deliciously delicious that i felt sinfully wicked\n",
      "word= the id= 4\n",
      "word= food id= 1644\n",
      "word= was id= 16\n",
      "word= so id= 38\n",
      "word= deliciously id= 6922\n",
      "word= delicious id= 6335\n",
      "word= that id= 15\n",
      "word= i id= 13\n",
      "word= felt id= 421\n",
      "sinfully Appended 2\n",
      "word= wicked id= 3799\n",
      "Prediction Probability for  RNN  =  0.40585133 Sentiment= Negative \n",
      "\n",
      "Prediction Probability for  CNN  =  0.88140005 Sentiment= Positive \n",
      "\n",
      "Prediction Probability for  LSTM  =  0.8970483 Sentiment= Positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def PredictSentiment(reviewList, ModelListTuple):\n",
    "    sentiment= {True: \"Positive\",\n",
    "               False: \"Negative\"}\n",
    "    Threshold = 0.5\n",
    "    for r in reviewList:\n",
    "        words = r.split()\n",
    "        review = []\n",
    "        print (\"review=\", r)\n",
    "        for word in words:\n",
    "          if word not in word2id: \n",
    "            review.append(2)\n",
    "            print (word, \"Appended 2\")\n",
    "          else:\n",
    "            if (word2id[word]) >= vocabulary_size:\n",
    "                print(\"got a word outside teh vocab_index\", word, word2id[word], \"breaking\")\n",
    "                break\n",
    "            print (\"word=\", word, \"id=\", word2id[word])\n",
    "            review.append(word2id[word]) \n",
    "        review = keras.preprocessing.sequence.pad_sequences([review],\n",
    "          truncating='pre', padding='pre', maxlen=max_words)\n",
    "        for i,m in enumerate(ModelListTuple):\n",
    "            if m[0] is not None:\n",
    "                prediction = m[0].predict(review)\n",
    "                print(\"Prediction Probability for \", ModelListTuple[i][1],\" = \", prediction[0][0], \"Sentiment=\", \n",
    "                      sentiment[prediction[0][0]>Threshold], \"\\n\")\n",
    "                \n",
    "PredictSentiment(reviewList,[(modelRNN, \"RNN\"), (modelCNN, \"CNN\"), (modelLSTM, \"LSTM\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Layers =  10\n",
      "<class 'keras.layers.embeddings.Embedding'>\n",
      "1\n",
      "(10000, 50)\n",
      "7695 4999 delicious deliciously\n",
      "[ 0.10139371  0.0796528   0.01947836 -0.01859712  0.03607012 -0.03364815\n",
      " -0.0507927   0.06992801  0.07061768 -0.06998959 -0.03293071 -0.01168939\n",
      "  0.02039245 -0.0270618   0.00965378  0.00759026 -0.01834591  0.0433141\n",
      "  0.03046671 -0.05842327  0.02599364  0.03251785 -0.06461079 -0.0654895\n",
      " -0.08622782  0.0263455  -0.07017993  0.02746692 -0.07096834  0.00587273\n",
      "  0.00692557 -0.04870772 -0.04295831 -0.00413456 -0.08027332 -0.01379449\n",
      "  0.0011602   0.07220315 -0.06396025  0.0145162   0.02385124  0.05906094\n",
      "  0.005033   -0.02897871  0.02116916  0.01621629 -0.02202742  0.01524497\n",
      "  0.00688786 -0.07370377]\n",
      "[-0.01447476  0.06404646  0.06271253 -0.0151422   0.05395892 -0.02797293\n",
      "  0.00327163  0.05282747  0.07158922 -0.03593937 -0.03513135  0.07226922\n",
      "  0.03979506  0.06547207 -0.00377295 -0.01637116 -0.06876769  0.01935615\n",
      "  0.0351758   0.01374875  0.07500815  0.05777484 -0.00703213 -0.00845107\n",
      " -0.08183787  0.07007829 -0.03150014  0.00724099 -0.04386286 -0.06490052\n",
      " -0.08845278 -0.05205308 -0.00898225 -0.02495421 -0.08273356 -0.07615967\n",
      " -0.07195004  0.02584936 -0.039366    0.069352    0.08291633  0.06500188\n",
      " -0.04746726 -0.07192054  0.06607953  0.05177934 -0.00631263 -0.04107828\n",
      "  0.08093518 -0.08077976]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "model=modelCNN\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "i = 0\n",
    "print (\"#Layers = \", len(layer_outputs))\n",
    "print (type(model.layers[0]))\n",
    "l = model.layers[0]\n",
    "print (len(l.get_weights()))\n",
    "print (l.get_weights()[0].shape)\n",
    "print (word2id[\"apple\"], word2id[\"orange\"], id2word[6335], id2word[6922])\n",
    "print (l.get_weights()[0][word2id[\"apple\"]])\n",
    "print (l.get_weights()[0][word2id[\"orange\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now use patience, and early stopping\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, restore_best_weights=True)\n",
    "#early1 = keras.callbacks.EarlyStopping(monitor =\"val_acc\", patience=1, baseline = 0.95)\n",
    "early1 = keras.callbacks.EarlyStopping(monitor =\"acc\", patience=1, restore_best_weights=True)\n",
    "\n",
    "callback_list = [early1,\n",
    "                keras.callbacks.ModelCheckpoint(filepath=\"my_mod1.h5\", monitor=\"val_loss\",\n",
    "                                               save_best_only=True),\n",
    "                keras.callbacks.TerminateOnNaN()\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 500) (1000,)\n"
     ]
    }
   ],
   "source": [
    "X_train3 = X_train2[:1000]\n",
    "Y_train3 = y_train2[:1000]\n",
    "print (X_train3.shape, Y_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 256 samples\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 0.2416 - acc: 0.9020 - val_loss: 0.5304 - val_acc: 0.8398\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.1172 - acc: 0.9530 - val_loss: 0.3208 - val_acc: 0.8750\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 0.1093 - acc: 0.9530 - val_loss: 0.2209 - val_acc: 0.9180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f09aeed0748>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train3, Y_train3, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=15, #num_epochs,\n",
    "             callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
